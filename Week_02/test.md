# 第四节课学习笔记：

由于下面两种原因：造成计算机最宝贵的资源被浪费掉，所以我们根据此优化产生了几种通信模型

![image-20210118150011571](Week_02/note model picture/image-20210118150011571.png)



cpu等待磁盘与网络数据，时间被浪费。从另一种角度看，我们的程序都执行在用户空间

![image-20210118151055559](Week_02/note model picture/image-20210118151055559.png)



基础介绍：-------------------------------------------------------------------------------------------------------------------------------------------

# 通信模型：

阻塞与非阻塞：是线程处理模型。

同步与异步：是通信模式

# 五种I/O模型：

![image-20210118151941059](Week_02/note model picture/image-20210118151941059.png)

## 阻塞式Io：

应用线程阻塞，cpu可以接收多线程进行线程切换

![image-20210118160551582](Week_02/note model picture/image-20210118160551582.png)

## 非阻塞式Io：

![image-20210118161414339](Week_02/note model picture/image-20210118161414339.png)

## 阻塞式I/0与非阻塞式I/0的区别：

应用程序调用阻塞 I/O 完成某个操作时，应用程序会被挂起，等待内核完成操作，感觉上应用程序像是被“阻塞”了一样。实际上，内核所做的事情是将 CPU 时间切换给其他有需要的进程，网络应用程序在这种情况下就会得不到 CPU 时间做该做的事情。

非阻塞 I/O 则不然，当应用程序调用非阻塞 I/O 完成某个操作时，内核立即返回，不会把 CPU 时间切换给其他进程，应用程序在返回后，可以得到足够的 CPU 时间继续完成其他事情。

## I/O多路复用（阻塞式）：

![image-20210118163916867](Week_02/note model picture/image-20210118163916867.png)

不过此种模型也有自己的缺点；：

1：每次调用 select，都需要把 fd（文件句柄，linux一切皆文件，无论线程，cpu等都可作为文件，这里是指socket） 集合从用户态拷贝到内核态，这个开销在 fd 很多时会很大
2：同时每次调用 select 都需要在内核遍历传递进来的所有 fd，这个开销在 fd 很多时也很大
3：select 支持的文件描述符数量太小了，默认是1024  

由于这些缺点，在linux2.6正式引入epoll：
（1）内核与用户空间共享一块内存
（2）通过回调解决遍历问题
（3）fd 没有限制，可以支撑10万连接  

### 套接字可读：

通过select 测试返回（唤醒用户线程），某个套接字准备好可读，第一种情况是套接字接收缓冲区有数据可以读，如果我们使用 read 函数去执行读操作，肯定不会被阻塞，而是会直接读到这部分数据。

第二种情况是对方发送了 FIN，使用 read 函数执行读操作，不会被阻塞，直接返回 0。

第三种情况是针对一个监听套接字而言的，有已经完成的连接建立，此时使用 accept 函数去执行不会阻塞，直接返回已经完成的连接。

第四种情况是套接字有错误待处理，使用 read 函数去执行读操作，不阻塞，且返回 -1。

总结：内核通知我们套接字有数据可以读了，使用 read 函数不会阻塞。

### 套接字可写：

第一种是套接字发送缓冲区足够大，如果我们使用套接字进行 write 操作，将不会被阻塞，直接返回。

第二种是连接的写半边已经关闭，如果继续进行写操作将会产生 SIGPIPE 信号。

第三种是套接字上有错误待处理，使用 write 函数去执行写操作，不阻塞，且返回 -1。

总结：内核通知我们套接字可以往里写了，使用 write 函数就不会阻塞。

select支持文件描述符：1024


### 实例：reactor模式（反应堆模型，事件驱动模型）：

![image-20210118165448383](Week_02/note model picture/image-20210118165448383.png)事件驱动模型，也被叫做反应堆模型（reactor），或者是 Event loop 模型。核心：

第一，它存在一个无限循环的事件分发线程，或者叫做 reactor 线程、Event loop 线程。这个事件分发线程的背后，就是 poll、epoll 等 I/O 分发技术的使用。

第二，所有的 I/O 操作都可以抽象成事件，每个事件必须有回调函数来处理。acceptor 上有连接建立成功、已连接套接字上发送缓冲区空出可以写、通信管道 pipe 上有数据可以读，这些都是一个个事件，通过事件分发，这些事件都可以一一被检测，并调用对应的回调函数加以处理。

### 事件驱动模式：

是解决高性能，高并发一种比较好的模式：

下面代表一个reactor线程同时负责分发 acceptor 的事件、已连接套接字的 I/O 事件。

![image-20210119105439322](Week_02/note model picture/image-20210119105439322.png)

上述设计模式的缺点：

和 I/O 事件处理相比，应用程序的业务逻辑处理是比较耗时的，比如 XML 文件的解析、数据库记录的查找、文件资料的读取和传输、计算型工作的处理等，这些工作相对而言比较独立，它们会拖慢整个反应堆模式的执行效率。所以，将这些 decode、compute、enode 型工作放置到另外的线程池中，和反应堆线程解耦，是一个比较明智的选择。反应堆线程只负责处理 I/O 相关的工作，业务逻辑相关的工作都被裁剪成一个一个的小任务，放到线程池里由空闲的线程来执行。当结果完成后，再交给反应堆线程，由反应堆线程通过套接字将结果发送出去。改进后模型如下：

![image-20210119110027614](Week_02/note model picture/image-20210119110027614.png)

### 主从reactor模式：

主 - 从这个模式的核心思想是，主反应堆线程只负责分发 Acceptor 连接建立，已连接套接字上的 I/O 事件交给 sub-reactor 负责分发。其中 sub-reactor 的数量，可以根据 CPU 的核数来灵活设置。而且，同一个套接字事件分发只会出现在一个反应堆线程中，这会大大减少并发处理的锁开销。

![image-20210119131421382](Week_02/note model picture/image-20210119131421382.png)



我们的主反应堆线程一直在感知连接建立的事件，如果有连接成功建立，主反应堆线程通过 accept 方法获取已连接套接字，接下来会按照一定的算法选取一个从反应堆线程，并把已连接套接字加入到选择好的从反应堆线程中。

### Netty使用的通信模式：见下面Netty部分



## 信号驱动（需要操作系统支持）：

信号驱动 IO 与 BIO 和 NIO 最大的区别就在于，在 IO 执行的数据准备阶段，不会阻塞用户进程。
如图所示：当用户进程需要等待数据的时候，会向内核发送一个信号，告诉内核我要什么数据，然后用户进程
就继续做别的事情去了，而当内核中的数据准备好之后，内核立马发给用户进程一个信号，说”数据准备好了，快来查收“，用户进程收到信号之后，会去查收数据。

信号驱动  引申-------》EDA（事件驱动架构）----》SEDA（分阶段事件驱动架构（多级缓冲））  

## 异步式 IO：

### 实例：proactor模式

异步 IO 真正实现了 IO 全流程的非阻塞。用户进程发出系统调用后立即返回，内核等待数据准备完成，然后将
数据拷贝到用户进程缓冲区，然后发送信号告诉用户进程 IO 操作执行完毕（与 SIGIO （信号io）相比，一个是发送信号告诉用户进程数据准备完毕，一个是 IO执行完毕），相当于操作系统把数据准备好，放在你需要的地方给你，这种只有windows下的iocp模式支持。  

# Netty简介：

## 特征：

异步 事件驱动 NIO

## 适用范围：

服务端，客户端，TCP/UDP

## 特性

高性能的协议服务器:
• 高吞吐
• 低延迟
• 低开销
• 零拷贝
• 可扩容  

## Netty与J2eeserver还有web区别？

j2ee定义了企业版规范（jboss，weblogic），把企业版规范去掉就是web版server（tomcat jetty）

httpserver（netty，ngix）返回简单的静态页面，js ，css图片  如果能够支持web规范，比如cookie，jsp，servlet。

## Netty里的基本概念（形象）

channel：打开一个连接，可执行读取/写入 IO 操作。Netty 对 Channel 的所有 IO 操作都是非阻塞的。  

ChannelFuture  ：只能查询操作的完成情况, 或者阻塞当前线程等待操作完成。Netty 封装一个 ChannelFuture 接口 ，可以将回调方法传给 ChannelFuture，在操作完成时自动执行  

Event & Handler Netty 基于事件驱动，事件和处理器可以关联到入站和出站数据流。  

 入站事件：

-  通道激活和停用
-  读操作事件
-  异常事件
-  用户事件

出站事件：

- 打开连接
- 关闭连接
- 写入数据
- 刷新数据           		

Encoder &Decoder：二进制与java对象数据，还有基本数据类型进行转化

ChannelPipeline  ：数据处理管道就是事件处理器链。 有顺序、同一 Channel 的出站处理器和入站处理器在同一个列表中。  

## Netty模式 ：主从reactor +work threads 模式

主 - 从 reactor 模式解决了 I/O 分发的高效率问题，那么 work threads 就解决了业务逻辑和 I/O 分发之间的耦合问题。Netty 里面提到的 worker 线程，其实就是我们这里说的从 reactor 线程，并不是处理具体业务逻辑的 worker 线程。

```java
//产生一个主reactor线程，只负责accetpor的对应处理 
EventLoopGroup bossGroup = new NioEventLoopGroup(1);
//产生一个从reactor线程，负责处理已连接套接字的I/O事件分发
 EventLoopGroup workerGroup = new NioEventLoopGroup(1);
try {           
    //标准的Netty初始，通过serverbootstrap完成线程池、channel以及对应的handler设置，注意这里将bossGroup和workerGroup作为参数设置           
    ServerBootstrap b = new ServerBootstrap();          
    b.group(bossGroup, workerGroup)            
        .channel(NioServerSocketChannel.class)             
        .handler(new LoggingHandler(LogLevel.INFO))             
        .childHandler(new TelnetServerInitializer(sslCtx));           
    //开启两个reactor线程无限循环处理            b.bind(PORT).sync().channel().closeFuture().sync();       

} 

```

![image-20210119133511052](Week_02/note model picture/image-20210119133511052.png)



这里将 decode、compute、encode 等 CPU 密集型的工作从 I/O 线程中拿走，这些工作交给 worker 线程池来处理，而且这些工作拆分成了一个个子任务进行。encode 之后完成的结果再由 sub-reactor 的 I/O 线程发送出去。

